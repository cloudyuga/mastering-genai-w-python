{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# AI Chat Analyzer - Simplified for Learning (No RAG)\n",
        "#\n",
        "# Objective: A simple Gradio application that uses the OpenAI API to analyze\n",
        "# a portion of an uploaded chat history file without a complex RAG pipeline.\n",
        "#\n",
        "\n",
        "# --- Step 1: Install Necessary Libraries ---\n",
        "# We only need 'gradio' for the web UI and 'openai' for the API.\n",
        "!pip install -q gradio openai\n",
        "\n",
        "# --- Step 2: Import Required Modules ---\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# --- Step 3: Backend Logic (The \"Brain\" of the App) ---\n",
        "\n",
        "def process_chat_request(user_question, chat_history, state_data):\n",
        "    \"\"\"\n",
        "    This function handles the user's question and generates a response.\n",
        "\n",
        "    Here's the simplified process (without RAG):\n",
        "    1.  Slice Context: It takes the full chat history and \"slices\" it, using\n",
        "        only the last N characters based on the \"Context Window Size\" slider.\n",
        "        This slice becomes the context.\n",
        "    2.  Construct Prompt: It creates a prompt for the AI, including the\n",
        "        sliced context, and instructs the AI to answer based only on that.\n",
        "    3.  Generate Answer: It sends the prompt to the OpenAI chat model\n",
        "        (gpt-3.5-turbo) to get an answer.\n",
        "    \"\"\"\n",
        "    # Retrieve the necessary data from the app's state.\n",
        "    api_key = state_data.get(\"api_key\")\n",
        "    temp = state_data.get(\"temp\")\n",
        "    chat_content = state_data.get(\"chat_content\")\n",
        "    context_size = state_data.get(\"context_size\")\n",
        "\n",
        "    # Basic validation.\n",
        "    if not all([api_key, temp is not None, chat_content, context_size]):\n",
        "        raise gr.Error(\"Configuration is incomplete. Please restart.\")\n",
        "    if not user_question:\n",
        "        raise gr.Error(\"Please enter a question.\")\n",
        "\n",
        "    # 1. Slice the chat content to get the context.\n",
        "    # We take the last `context_size` characters from the file.\n",
        "    # For example, if the size is 5000, we send the last 5000 characters.\n",
        "    context_to_use = chat_content[-int(context_size):]\n",
        "\n",
        "    # 2. Construct the prompt for the LLM.\n",
        "    prompt = f\"\"\"\n",
        "    You are a helpful assistant that analyzes a chat history. Based ONLY on the provided chat context below,\n",
        "    answer the user's question. Do not use any external knowledge. If the answer is not in the context,\n",
        "    state that you cannot find the answer in the provided chat history.\n",
        "\n",
        "    --- CHAT CONTEXT ---\n",
        "    {context_to_use}\n",
        "    --- END OF CONTEXT ---\n",
        "\n",
        "    QUESTION:\n",
        "    \"{user_question}\"\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # 3. Call the OpenAI API to generate the response.\n",
        "        client = OpenAI(api_key=api_key)\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert chat analyst.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=temp,\n",
        "        )\n",
        "        bot_response = completion.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"An API error occurred: {e}\")\n",
        "\n",
        "    # Add the conversation turn to the chat history.\n",
        "    chat_history.append((user_question, bot_response))\n",
        "    return \"\", chat_history\n",
        "\n",
        "# --- Step 4: Gradio User Interface (The \"Look and Feel\") ---\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"emerald\", secondary_hue=\"emerald\"), title=\"Chat Analyzer\") as demo:\n",
        "    # 'State' object stores data that persists across interactions.\n",
        "    app_state = gr.State({})\n",
        "\n",
        "    # --- Page 1: Welcome Screen ---\n",
        "    with gr.Column(visible=True) as welcome_page:\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            <div style='text-align: center; font-family: \"Garamond\", serif; padding-top: 50px;'>\n",
        "                <h1 style='font-size: 4em;'>Chat with your History</h1>\n",
        "                <p style='font-size: 1.5em; color: #555;'>Analyze any chat export using OpenAI.</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        )\n",
        "        gr.HTML(\n",
        "            \"\"\"\n",
        "            <div style='text-align: center; padding: 30px;'>\n",
        "                <img src='https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExd2Vjb3M2eGZzN2FkNWZpZzZ0bWl0c2JqZzZlMHVwZ2l4b2t0eXFpcyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/YWjDA4k2n6d5Ew42zC/giphy.gif'\n",
        "                     style='max-width: 400px; margin: auto; border-radius: 20px; box-shadow: 0 8px 16px rgba(0,0,0,0.1);' />\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        )\n",
        "        with gr.Row():\n",
        "            enter_button = gr.Button(\"âœ¨ Get Started âœ¨\", variant=\"primary\", scale=1)\n",
        "\n",
        "    # --- Page 2: Configuration Screen ---\n",
        "    with gr.Column(visible=False) as config_page:\n",
        "        gr.Markdown(\"<h1 style='text-align: center;'>Setup Your Chat Environment</h1>\")\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### 1. OpenAI API Key\")\n",
        "                api_key_input = gr.Textbox(label=\"API Key\", type=\"password\", placeholder=\"Enter your OpenAI key here...\")\n",
        "                gr.Markdown(\"### 2. Upload Chat File\")\n",
        "                chat_file_upload = gr.File(label=\"Upload Any .txt Chat Export\")\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### 3. Customize AI Parameters\")\n",
        "                # This slider now controls how many characters from the end of the\n",
        "                # file are used as context for the AI.\n",
        "                context_slider = gr.Slider(\n",
        "                    500, 20000, value=5000, step=500,\n",
        "                    label=\"Context Window Size (Characters)\"\n",
        "                )\n",
        "                temp_slider = gr.Slider(0.0, 1.0, value=0.5, step=0.1, label=\"Temperature (Creativity)\")\n",
        "        lets_chat_button = gr.Button(\"ðŸ’¬ Let's Chat! ðŸ’¬\", variant=\"primary\")\n",
        "\n",
        "    # --- Page 3: Chat Interface ---\n",
        "    with gr.Column(visible=False) as chat_page:\n",
        "        gr.Markdown(\"<h1 style='text-align: center;'>Chat Analyzer</h1>\")\n",
        "        chatbot_ui = gr.Chatbot(height=600, bubble_full_width=False, label=\"Chatbot\")\n",
        "        with gr.Row():\n",
        "            user_input_box = gr.Textbox(placeholder=\"Ask a question about your chat...\", scale=5, label=\"Your Question\")\n",
        "            submit_button = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
        "\n",
        "    # --- Step 5: Event Handling (Connecting UI to Backend) ---\n",
        "\n",
        "    def go_to_config():\n",
        "        \"\"\"Function to switch from the welcome page to the config page.\"\"\"\n",
        "        return {\n",
        "            welcome_page: gr.Column(visible=False),\n",
        "            config_page: gr.Column(visible=True)\n",
        "        }\n",
        "\n",
        "    def go_to_chat(api_key, chat_file, context_size, temp):\n",
        "        \"\"\"\n",
        "        Function to save the config and switch to the chat page.\n",
        "        \"\"\"\n",
        "        if not api_key:\n",
        "            raise gr.Error(\"API Key is required.\")\n",
        "        if chat_file is None:\n",
        "            raise gr.Error(\"A chat file must be uploaded.\")\n",
        "\n",
        "        # Read the content of the uploaded file.\n",
        "        with open(chat_file.name, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Save the configuration into the app state.\n",
        "        # Note: We are now storing the raw file content directly.\n",
        "        new_state = {\n",
        "            \"api_key\": api_key,\n",
        "            \"chat_content\": content,\n",
        "            \"context_size\": context_size,\n",
        "            \"temp\": temp,\n",
        "        }\n",
        "\n",
        "        # Return the new state and the visibility updates for the pages.\n",
        "        return (\n",
        "            new_state,\n",
        "            gr.Column(visible=False), # Hide config page\n",
        "            gr.Column(visible=True)   # Show chat page\n",
        "        )\n",
        "\n",
        "    # This section \"wires up\" the buttons to their respective functions.\n",
        "    enter_button.click(fn=go_to_config, inputs=None, outputs=[welcome_page, config_page])\n",
        "\n",
        "    lets_chat_button.click(\n",
        "        fn=go_to_chat,\n",
        "        # The context_slider is now an input.\n",
        "        inputs=[api_key_input, chat_file_upload, context_slider, temp_slider],\n",
        "        outputs=[app_state, config_page, chat_page]\n",
        "    )\n",
        "\n",
        "    submit_button.click(\n",
        "        fn=process_chat_request,\n",
        "        inputs=[user_input_box, chatbot_ui, app_state],\n",
        "        outputs=[user_input_box, chatbot_ui]\n",
        "    )\n",
        "\n",
        "    user_input_box.submit(\n",
        "        fn=process_chat_request,\n",
        "        inputs=[user_input_box, chatbot_ui, app_state],\n",
        "        outputs=[user_input_box, chatbot_ui]\n",
        "    )\n",
        "\n",
        "# --- Step 6: Launch the App ---\n",
        "print(\"\\nLaunching Gradio App...\")\n",
        "demo.launch(debug=True, share=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2972168350.py:133: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot_ui = gr.Chatbot(height=600, bubble_full_width=False, label=\"Chatbot\")\n",
            "/tmp/ipython-input-2972168350.py:133: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot_ui = gr.Chatbot(height=600, bubble_full_width=False, label=\"Chatbot\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Launching Gradio App...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://22e45ef1d599138162.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://22e45ef1d599138162.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://22e45ef1d599138162.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "whsGx6I8LWHi",
        "outputId": "f05612a1-7186-4796-e00f-3633ec091693"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}