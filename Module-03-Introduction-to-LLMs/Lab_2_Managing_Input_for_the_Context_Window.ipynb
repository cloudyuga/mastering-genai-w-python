{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Lab 3: Managing Input for the Context Window (Chunking)\n",
        "#\n",
        "# Objective: Learn how to process documents that are larger than the model's\n",
        "# context window by breaking them into smaller pieces (\"chunks\").\n",
        "#\n",
        "# You can run this notebook directly in Google Colab.\n",
        "#\n",
        "\n",
        "# Step 1: Install and configure the API\n",
        "!pip install -q google-generativeai\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    print(\"Successfully configured the API key!\")\n",
        "except ImportError:\n",
        "    print(\"Not in a Colab environment. Please set the GEMINI_API_KEY environment variable.\")\n",
        "\n",
        "\n",
        "# Step 2: Define the Model\n",
        "# We can use a powerful model for this task.\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "print(f\"Using model: {model.model_name}\")\n",
        "\n",
        "# Step 3: Create a \"long document\" that we need to process.\n",
        "# In a real scenario, this could be a PDF, a book, or a long report.\n",
        "# For this lab, we'll just create a long string.\n",
        "\n",
        "part1 = \"\"\"\n",
        "Project Gutenberg's \"A Tale of Two Cities\", by Charles Dickens.\n",
        "It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other wayâ€”in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\n",
        "This part of the story introduces the contrasting states of England and France in 1775.\n",
        "\"\"\"\n",
        "\n",
        "part2 = \"\"\"\n",
        "The story then moves to describe the mail-coach journey from London to Dover. The passengers are suspicious of each other, fearing robbers. Mr. Jarvis Lorry, an elderly gentleman from Tellson's Bank, receives a cryptic message: \"Wait at Dover for Mam'selle.\" He responds with an equally cryptic message: \"Recalled to life.\" This exchange sets up the central mystery of the novel concerning Doctor Manette.\n",
        "Dr. Manette is a French physician who was secretly imprisoned in the Bastille for 18 years.\n",
        "\"\"\"\n",
        "\n",
        "part3 = \"\"\"\n",
        "Lucie Manette, a young woman who believed she was an orphan, is brought from London to Paris by Mr. Lorry. He reveals to her that her father is alive and has been released. They go to the garret of a wine-shop in the Parisian suburb of Saint Antoine, owned by Monsieur and Madame Defarge. There, they find Dr. Manette, who has lost his memory and spends his time compulsively making shoes, a skill he learned in prison.\n",
        "The main characters are Lucie Manette, Charles Darnay, and Sydney Carton.\n",
        "\"\"\"\n",
        "\n",
        "long_document = part1 + part2 + part3\n",
        "\n",
        "# Let's verify the total token count.\n",
        "total_tokens = model.count_tokens(long_document).total_tokens\n",
        "print(f\"The total document has {total_tokens} tokens.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# Step 4: The Strategy - Chunking\n",
        "# We can't \"adjust\" the model's window, but we can adjust our input.\n",
        "# The strategy is to split the document into chunks that WILL fit.\n",
        "# For this example, we'll set a small chunk size to see the process clearly.\n",
        "\n",
        "# We will split the text by paragraphs (double newlines)\n",
        "chunks = long_document.split('\\n\\n')\n",
        "\n",
        "print(f\"The document was split into {len(chunks)} chunks.\")\n",
        "for i, chunk in enumerate(chunks):\n",
        "    chunk_tokens = model.count_tokens(chunk).total_tokens\n",
        "    print(f\"  - Chunk {i+1}: {chunk_tokens} tokens. Content: '{chunk.strip()[:50]}...'\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# Step 5: Process Each Chunk Individually\n",
        "# Let's perform a task: For each chunk, we'll ask the model to pull out the key names and places.\n",
        "\n",
        "all_key_points = []\n",
        "print(\"\\n--- Processing each chunk individually ---\")\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    if not chunk.strip(): # Skip empty chunks\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nProcessing Chunk {i+1}...\")\n",
        "    prompt = f\"\"\"\n",
        "    Based on the following text, extract the key character names and locations mentioned.\n",
        "    If no names or locations are mentioned, say 'None'.\n",
        "\n",
        "    TEXT:\n",
        "    \"{chunk}\"\n",
        "\n",
        "    NAMES AND LOCATIONS:\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        print(f\"  > Model Response: {response.text.strip()}\")\n",
        "        all_key_points.append(response.text.strip())\n",
        "    except Exception as e:\n",
        "        print(f\"  > Could not process chunk: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 50)\n",
        "\n",
        "\n",
        "# Step 6: Combine the Results (Map-Reduce style)\n",
        "# Now that we have processed each chunk, we can combine the results to get a\n",
        "# summary of the entire document. This is a form of the \"Map-Reduce\" pattern.\n",
        "\n",
        "print(\"\\n--- Combining the results for a final summary ---\")\n",
        "\n",
        "# Join all the extracted points into one text\n",
        "combined_points = \"\\n\".join(all_key_points)\n",
        "\n",
        "final_summary_prompt = f\"\"\"\n",
        "Based on the following list of key points extracted from a document, write a single, coherent paragraph summarizing the main events and characters.\n",
        "\n",
        "EXTRACTED POINTS:\n",
        "{combined_points}\n",
        "\n",
        "FINAL SUMMARY:\n",
        "\"\"\"\n",
        "\n",
        "final_response = model.generate_content(final_summary_prompt)\n",
        "\n",
        "print(\"\\nFinal Combined Summary of the entire document:\")\n",
        "print(final_response.text)\n",
        "\n",
        "\n",
        "# --- Conclusion of Lab 3 ---\n",
        "# You've learned:\n",
        "# 1. The primary strategy for handling large documents is \"chunking\" - splitting them into smaller parts.\n",
        "# 2. You can process each chunk individually to perform tasks like extraction or summarization.\n",
        "# 3. The results from each chunk can then be combined in a final step to create a comprehensive result for the entire document (a \"Map-Reduce\" approach).\n",
        "# 4. This technique allows you to overcome the context window limitation for very large inputs."
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully configured the API key!\n",
            "Using model: models/gemini-1.5-flash\n",
            "The total document has 405 tokens.\n",
            "--------------------------------------------------\n",
            "The document was split into 3 chunks.\n",
            "  - Chunk 1: 180 tokens. Content: 'Project Gutenberg's \"A Tale of Two Cities\", by Cha...'\n",
            "  - Chunk 2: 108 tokens. Content: 'The story then moves to describe the mail-coach jo...'\n",
            "  - Chunk 3: 115 tokens. Content: 'Lucie Manette, a young woman who believed she was ...'\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Processing each chunk individually ---\n",
            "\n",
            "Processing Chunk 1...\n",
            "  > Model Response: NAMES: None\n",
            "\n",
            "LOCATIONS: England, France\n",
            "\n",
            "Processing Chunk 2...\n",
            "  > Model Response: NAMES: Mr. Jarvis Lorry, Mam'selle, Doctor Manette\n",
            "\n",
            "LOCATIONS: London, Dover, Tellson's Bank, Bastille\n",
            "\n",
            "Processing Chunk 3...\n",
            "  > Model Response: NAMES: Lucie Manette, Mr. Lorry, Monsieur Defarge, Madame Defarge, Dr. Manette, Charles Darnay, Sydney Carton\n",
            "\n",
            "LOCATIONS: London, Paris, Saint Antoine (Parisian suburb), garret of a wine-shop\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Combining the results for a final summary ---\n",
            "\n",
            "Final Combined Summary of the entire document:\n",
            "The story follows Mr. Jarvis Lorry of Tellson's Bank in London as he travels to France to rescue Dr. Manette, imprisoned in the Bastille.  Dr. Manette is reunited with his daughter, Lucie, in London, where they also encounter Charles Darnay, a French aristocrat.  The narrative shifts between London and the volatile streets of Paris, particularly the Saint Antoine suburb, where the revolutionaries Monsieur and Madame Defarge play pivotal roles.  The intertwined lives of Lucie, Darnay, and Sydney Carton unfold against the backdrop of the French Revolution, with events culminating in the Parisian garret of a wine-shop, revealing the far-reaching consequences of the revolution and the characters' complex relationships.\n",
            "\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "i3EU_vLKQ-46",
        "outputId": "81eaa8bc-4fdb-415c-ed6e-c45ce238b652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        }
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3855ed13"
      },
      "source": [
        "print(\"\\n--- Processing with a smaller, custom context window ---\")\n",
        "\n",
        "# Define a hypothetical smaller context window size (in characters)\n",
        "custom_context_window_size = 200\n",
        "\n",
        "# Truncate the document to fit within the custom window\n",
        "truncated_document = long_document[:custom_context_window_size]\n",
        "\n",
        "truncated_document_prompt = f\"\"\"\n",
        "Based on the following text, extract the key character names and locations mentioned.\n",
        "If no names or locations are mentioned, say 'None'.\n",
        "\n",
        "TEXT:\n",
        "\"{truncated_document}\"\n",
        "\n",
        "NAMES AND LOCATIONS:\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    # Process the truncated document\n",
        "    truncated_response = model.generate_content(truncated_document_prompt)\n",
        "    print(f\"  > Model Response (Truncated Document): {truncated_response.text.strip()}\")\n",
        "except Exception as e:\n",
        "    print(f\"  > Could not process truncated document: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "341b9fac"
      },
      "source": [
        "### Processing with a smaller, custom context window\n",
        "\n",
        "Note: The model's actual context window size is fixed. We are simulating the effect of a smaller window by truncating the input string to fit within a hypothetical smaller window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee8198bf"
      },
      "source": [
        "print(\"\\n--- Attempting to process the full document ---\")\n",
        "\n",
        "full_document_prompt = f\"\"\"\n",
        "Based on the following text, extract the key character names and locations mentioned.\n",
        "If no names or locations are mentioned, say 'None'.\n",
        "\n",
        "TEXT:\n",
        "\"{long_document}\"\n",
        "\n",
        "NAMES AND LOCATIONS:\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    # Attempt to process the full document\n",
        "    full_response = model.generate_content(full_document_prompt)\n",
        "    print(f\"  > Model Response (Full Document): {full_response.text.strip()}\")\n",
        "except Exception as e:\n",
        "    print(f\"  > Could not process full document: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d71647c2"
      },
      "source": [
        "### Attempting to process the full document (may exceed context window)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21c71ccf"
      },
      "source": [
        "## Demonstrating the effect of context window size\n",
        "\n",
        "Let's see what happens when we try to process the full document without chunking, and then with a smaller, custom context window size."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}